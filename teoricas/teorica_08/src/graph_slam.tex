\begin{frame}
    \frametitle{Graph-SLAM}
    
    Graph-SLAM: Construct a graph and find a configuration of nodes that minimizes the error introduced by constraints (edges)
    
    \begin{itemize}
    \item A graph is used to represent the problem.
    \item The nodes represent poses or locations of landmarks.
    \item Edges are landmark observations or odometry measurements
    \item Minimization optimizes robot poses and landmark placement
    \item Observing previously viewed areas generates constraints in the graph
    \end{itemize}
    
    \begin{figure}
    \subfloat[]
    {
    \fbox{\includegraphics[width=0.25\textwidth]{images/slam-landmarks.pdf}}
    }\hspace{1em}
    \subfloat[]
    {
    \fbox{\includegraphics[width=0.375\textwidth]{images/factor_graph.pdf}}
    }
    \end{figure}
    
    \end{frame}
    
    \begin{frame}
    \frametitle{Factor-Graph}
    \note{Information taken from https://youtu.be/uuiaqGLFYa4}
    
    \begin{block}{Factor-Graph}
    A Factor-graph is a mathematical term, a bipartite graph representing the factorization of a function. This means that we can take a function, for example, $g(.)$ and decompose it by the product of functions $f(.)$,
    
    \begin{equation*}
    g(X_{1}, \dots, X_{n}) = \prod_{i} f_{i}(S_{i}) \quad \text{con} \quad S_{i} \subseteq \{ X_{1},\dots, X_{n} \}
    \end{equation*}
    \end{block}
    
    \begin{figure}[!h]
    \includegraphics[width=0.7\textwidth]{images/factor_graph_example.pdf}
    \end{figure}
    
    \note{For example, if the function g depends on 10 variables, we can decompose it into the product of several functions f, where each function f depends on a subset of those variables.}
    
    \end{frame}
    
    \begin{frame}
    \frametitle{Factor-Graph}
    \note{Information taken from https://youtu.be/uuiaqGLFYa4}
    
    \begin{itemize}
    \item Factor-graphs allow us to represent a joint probability distribution (a distribution that governs all variables) as a product of smaller probabilities (which depend on a smaller number of variables).
    \item Like Bayes networks or Markov networks, we can use Factor-graphs to describe how variables depend on each other. And we can run different algorithms on these Factor-graphs to efficiently infer information.
    \item An example of an algorithm that works on a Factor-graph is the Sum-Product Algorithm for computing marginal distributions (distributions that only depend on a subset of variables).
    \item In the context of robotics, Factor-graphs are used to specify least squares problems. The factor graph allows us to represent how certain states depend on or are related to each other based on the information we have from sensor measurements (stored in the factors).
    \end{itemize}
    
    \note{For example, if function g depends on 10 variables, we can decompose it into the product of several functions f, where each function f depends on a subset of those variables.}
    
    \end{frame}
    
    \begin{frame}
    \frametitle{Graph-based SLAM using Pose-Graph}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
    \begin{itemize}
    \item The constraints connect the robot's poses as it moves.
    \item The constraints are noisy.
    \end{itemize}
    
    \begin{figure}[!h]
    \includegraphics[width=0.7\textwidth]{images/pose_graph_example.pdf}
    \end{figure}
    
\end{frame}

\begin{frame}
    \frametitle{Graph-based SLAM using Pose-Graph}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
   
    \begin{itemize}
    \item By observing previously seen areas, restrictions are generated between non-consecutive poses (\emph{Loop Closure})
    \end{itemize}
   
    \begin{figure}[!h]
    \includegraphics[width=0.4\textwidth]{images/pose_graph_loop_example.pdf}
    \end{figure}
   
   \end{frame}
   
   \begin{frame}[fragile]
    \frametitle{2D Pose-Graph with LiDAR}
    \note{Video taken from https://youtu.be/E6IvbjZA7Ao}
   
    \begin{center}
   \movie[loop]{\includegraphics[width=0.5\columnwidth]{./images/pose_graph_2d_video.jpg}}{./videos/pose_graph_2d.mp4}
   \end{center}
   
   \end{frame}
   
   \begin{frame}
   \frametitle{Graph-based SLAM using Pose-Graph}
   \note{Information taken from https://youtu.be/uHbRKvD8TWg}
   
   \begin{columns}
   \begin{column}{0.5\textwidth}
   \begin{itemize}
   \item<1-2> Each node is a robot pose along with its laser measurement (there are no landmarks)
   \item<1-2> Each edge corresponds to a spatial constraint between the nodes it relates. \item<3-> Once we have the graph, we obtain the most probable map by correcting the nodes
    \item<4-> like this...
    \item<5> We draw the map based on the corrected poses
    \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth} %%<--- here
    \only<1>{\includegraphics[width=0.7\textwidth]{images/pose_graph_map.png}}
    \only<2>{\includegraphics[width=0.7\textwidth]{images/pose_graph_constraints_with_map.png}}
    \only<3>{\includegraphics[width=0.7\textwidth]{images/pose_graph_constraints.png}}
    \only<4>{\includegraphics[width=0.5\textwidth]{images/pose_graph_optimized.png}}
    \only<5>{\includegraphics[width=0.5\textwidth]{images/pose_graph_optimized_with_map.png}}
    \end{column}
    \end{columns}
   
   \end{frame}
   
   \begin{frame}
    \frametitle{The graph in pose-graph}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
   
    \begin{itemize}
    \item Consists of $n$ nodes $\stateBold = \stateBold_{1:n}$
    \item Each $\stateBold_{i}$ is a pose of the robot at time $t_{i}$
    \item A constraint/edge exists between node $\stateBold_{i}$ and $\stateBold_{j}$ if...
   \end{itemize}
   
   \begin{figure}[!h]
   \includegraphics[width=0.3\textwidth]{pose_graph_loop_example.pdf}
   \end{figure}
   
   \end{frame}
   
   \begin{frame}
   \frametitle{Create an edge if...}
   \note{Information taken from https://youtu.be/uHbRKvD8TWg}
   
   \begin{itemize}
   \item A constraint/edge exists between node $\stateBold_{i}$ and $\stateBold_{j}$ if the robot moves from $\stateBold_{i}$ to $\stateBold_{i+1}$
   \item The edges correspond to odometry
   \end{itemize}
   
   \begin{figure}[!h]
   \includegraphics[width=0.44\textwidth]{pose_graph_odometry_edge.pdf}
   \end{figure}
   
   \end{frame}
   
   \begin{frame}
   \frametitle{Create an edge if...}
   \note{Information taken from https://youtu.be/uHbRKvD8TWg}
   
   \begin{itemize}
   \item<1-> A constraint/edge exists between node $\stateBold_{i}$ and $\stateBold_{j}$ if the robot observes the same part of the environment from $\stateBold_{i}$ and from $\stateBold_{j}$
   \item<2> We construct a {\bf virtual constraint} between the position of $\stateBold_{j}$ as seen from $\stateBold_{i}$
   \end{itemize}
   \only<1>{
   \begin{figure}
   \includegraphics[width=0.44\textwidth]{pose_graph_edge_lidar.pdf}
   \end{figure}
   }
   \only<2>{
   \begin{figure}
   \includegraphics[width=0.44\textwidth]{pose_graph_edge_lidar2.pdf}
   \end{figure}
   }
   
   \end{frame}
   
   \begin{frame}
   \frametitle{The edge information matrix}
   \note{Information taken from https://youtu.be/uHbRKvD8TWg}
   \begin{itemize}
   \item Observations are noisy
   \item The information matrix $\informationMatrix_{ij}$ for each edge encodes its uncertainty
   \item The larger the $\informationMatrix_{ij}$, the more important the edge is in the optimization.
   \end{itemize}
   
   \end{frame}
   
   \begin{frame}
    \frametitle{The edge information matrix}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    \begin{figure}[!h]
    \includegraphics[width=0.7\textwidth]{images/factor_graph_edge_example.pdf}
    \end{figure}
   
    Aim:
    \begin{equation*}
    \stateBold^{*} = \argmin_{\stateBold} \sum _{ij} \error^{\top}_{ij} \informationMatrix_{ij} \error_{ij}
    \end{equation*}
   
\end{frame}



\begin{frame}
    \frametitle{Least Squares in SLAM}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    We can minimize the error using least squares (\emph{Least Square})
    \begin{align*}
    \state^{*} &= \argmin_{\stateBold} \sum_{ij} \error^{\top}_{ij}(\stateBold_{i},\stateBold_{j}) \informationMatrix_{ij} \error_{ij}(\stateBold_{i},\stateBold_{j})\\
    &= \argmin_{\stateBold} \sum_{k} \error^{\top}_{k}(\stateBold) \informationMatrix_{k} \error_{k}(\stateBold)
    \end{align*}
    
    The {\bf state vector} is the Concatenation of the pose nodes $\stateBold = (\stateBold_{1}^{\top} \stateBold_{2}^{\top} \dots \stateBold_{n}^{\top})$. Each node is a pose (position or orientation).
    
    \vspace{2em}
    {\bf We need to define the error function...}
    \end{frame}
    
    \section{Least Squares}
    \input{src/least_squares.tex}
    
    \begin{frame}
    \frametitle{Error Function}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
    \begin{itemize}
    \item Error Function for a Single Constraint
    \begin{figure}
    \includegraphics[width=0.44\textwidth]{pose_graph_error_function.pdf}
    \end{figure}
    \footnotetext{$t2v(.)$ maps transformations to vectors}
    
    \item Error as a Function of a Full State Vector
    
    \begin{equation*}
    \error_{ij}(\state) = t2v(\inverse{\observationBold}_{ij}(\inverse{\stateBold}_{i}\stateBold_{j}))
    \end{equation*}
    
    \item The error takes the value 0 when
    
    \begin{equation*}
    \observationBold_{ij} = (\inverse{\stateBold}_{i}\stateBold_{j})
    \end{equation*}
    
    \end{itemize}
    \end{frame}
    
    \begin{frame}
    \frametitle{Linearizing the error function}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
    \begin{itemize}
    \item We can approximate the error around an initial estimate $\stateBold$ through a Taylor expansion
    \end{itemize}
    
    \begin{equation*}
    \error_{ij}(\stateBold + \vec{\Delta}\stateBold) \simeq \error_{ij}(\stateBold) + \jacobian_{ij}\vec{\Delta}\stateBold \quad \text{con} \quad \jacobian_{ij} = \dfrac{\partial\error_{ij}(\stateBold)}{\partial\stateBold}
    \end{equation*}
    
    \end{frame}
    
    \begin{frame}
    \frametitle{Derivative of the error function}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    \begin{itemize}
    \item<1-> Question: Does an error term $\error_{ij}(\stateBold)$ depend on all state variables?
    
    \only<2->{No, it only depends on $\stateBold_{i}$ and $\stateBold_{j}$}
    
    \item<3-> Question: Are there any consequences for the structure of the Jacobian?
    
    \only<4->{
     Yes, it will be different from zero only in the rows corresponding to $\stateBold_{i}$ and $\stateBold_{j}$
    
     \begin{align*}
     \dfrac{\partial\error_{ij}(\stateBold)}{\partial\stateBold} &=
     \begin{bmatrix}
     0 & \dots & \dfrac{\partial\error_{ij}(\stateBold_{i})}{\partial\stateBold_{i}} & \dots & 0 & \dots & \dfrac{\partial\error_{ij}(\stateBold_{j})}{\partial\stateBold_{j}} & \dots & 0
     \end{bmatrix} \\
     \jacobian_{ij} &=
     \begin{bmatrix}
     0 & \dots & \vec{A}_{ij} & \dots & 0 & \dots & \vec{B}_{ij} & \dots & 0
     \end{bmatrix}
     \end{align*}
     }
    
    
     \end{itemize}
    
    \end{frame}
    
    
    \begin{frame}
     \frametitle{Jacobians and the sparse problem}
     \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
     \begin{itemize}
     \item The error $\error_{ij}(\stateBold)$ depends only on the parameter blocks $\stateBold_{i}$ and $\stateBold_{j}$
    
     \begin{equation*}
     \error_{ij}(\stateBold) =\error_{ij}(\stateBold_{i} ,\stateBold_{j})
     \end{equation*}
    
     \item The Jacobian will be zero everywhere except in the columns $\stateBold_{i}$ and $\stateBold_{j}$
    
     \begin{equation*}
     \jacobian_{ij} =
     \begin{bmatrix}
     0 & \dots & 0 & \dfrac{\partial\error_{ij}(\stateBold_{i})}{\partial\stateBold_{i}} & 0 & \dots & 0 & \dfrac{\partial\error_{ij}(\stateBold_{j})}{\partial\stateBold_{j}} & 0 & \dots & 0
     \end{bmatrix}
     \end{equation*}
    
     \end{itemize}
    
     {\bf This allows us to solve SLAM efficiently!}
    
\end{frame}


\begin{frame}
    \frametitle{Consequence of this being a sparse problem}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
    \begin{itemize}
    \item We need to compute the coefficient vector.
    \begin{align*}
    \linearSystemb^{\top} &= \sum_{ij} \linearSystemb_{ij}^{\top} = \sum_{ij} \error_{ij}^{\top}\Omega_{ij}\jacobian_{ij}\\
    \linearSystemH &= \sum_{ij} \linearSystemH_{ij} = \sum_{ij} \jacobian_{ij}^{\top}\Omega_{ij}\jacobian_{ij}
    \end{align*}
    \item The sparse structure of $\jacobian_{ij}$ will result in a sparse structure of $\linearSystemH$
    \item This structure reflects the graph's adjacency matrix.
    \end{itemize}
    
    \end{frame}
    
    \begin{frame}
    \frametitle{Illustration of the structure}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
    \begin{figure}[!h]
    \includegraphics[width=0.9\textwidth]{linear_system_sparsity.pdf}
    \end{figure}
    
    \end{frame}
    
    \begin{frame}
    \frametitle{Consequence of it being a sparse problem}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
    \begin{figure}[!h]
    \includegraphics[width=0.9\textwidth]{linear_system_sparsity2.pdf} 
    \end{figure}
    
    \end{frame}
    
    \begin{frame}
     \frametitle{Consequence of it being a dispersed problem}
     \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
     \begin{itemize}
     \item An edge contributes to the linear system through $\linearSystemb_{ij}$ and $\linearSystemH_{ij}$
     \item The coefficient vector is
    
     \begin{align*}
     \linearSystemb_{ij}^{\top} &= \error_{ij}^{\top}\informationMatrix_{ij}\jacobian_{ij}\\
     &= \error_{ij}^{\top}\informationMatrix_{ij}
     \begin{bmatrix}
     0 & \dots & \vec{A}_{ij} & \dots & \vec{B}_{ij} & \dots & 0
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
    0 & \dots & \error_{ij}^{\top}\informationMatrix_{ij}\vec{A}_{ij} & \dots & \error_{ij}^{\top}\informationMatrix_{ij}\vec{B}_{ij} & \dots & 0
    \end{bmatrix}
    \end{align*}
    \item It is nonzero only at the corresponding indices of $\stateBold_{i}$ and $\stateBold_{j}$
    \end{itemize}
    
    \end{frame}
    
    \begin{frame}
    \frametitle{Consequence of it being a sparse problem}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    \small
    \begin{itemize}
    \item The coefficient matrix of an edge is
     \begin{align*}
     \linearSystemH_{ij}^{\top} &= \jacobian_{ij}^{\top}\informationMatrix_{ij}\jacobian_{ij}\\
     &=
     \begin{bmatrix}
     \vdots \\
     \vec{A}_{ij}^{\top}\\
     \vdots \\
     \vec{B}_{ij}^{\top}\\
     \vdots
     \end{bmatrix} \informationMatrix_{ij}
     \begin{bmatrix}
     \cdots & \vec{A}_{ij} & \cdots & \vec{B}_{ij} & \cdots
     \end{bmatrix}\\
     &=
     \begin{bmatrix}
     0 & \cdots & 0 & \cdots & 0 \\
     \vdots & \vec{A}_{ij}^{\top}\informationMatrix_{ij}\vec{A}_{ij} & \vdots & \vec{A}_{ij}^{\top}\informationMatrix_{ij}\vec{B}_{ij} & \vdots\\
     0 & \cdots & 0 & \cdots & 0 \\
     \vdots & \vec{B}_{ij}^{\top}\informationMatrix_{ij}\vec{A}_{ij} & \vdots & \vec{B}_{ij}^{\top}\informationMatrix_{ij}\vec{B}_{ij} & \vdots\\
     0 & \cdots & 0 & \cdots & 0
     \end{bmatrix}
     \end{align*}
     \item Is non-zero in related blocks with $i,j$
    \end{itemize}
    
    \end{frame}
    
    \begin{frame}
    \frametitle{Summary of the sparse problem}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
    \begin{itemize}
    \item An edge ij only contributes to
    \begin{itemize}
    \item the i-th and j-th blocks of $\linearSystemb_{ij}$
    \item the blocks $ii$, $jj$, $ij$, and $ji$ of $\linearSystemH_{ij}$
    \end{itemize}
    \item The resulting system is sparse.
    \item The system can be computed by summing the contributions of each edge.
    \item Different \emph{solvers} can be used.
    \begin{itemize}
    \item Sparse Cholesky decomposition.
    \item Conjugate gradient. \item many more...
     \end{itemize}
    
     \end{itemize}
    
    
\end{frame}

\begin{frame}
    \frametitle{The Linear System}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
   
    \begin{itemize}
    \item Vector of state increments
    \begin{equation*}
    \Delta\stateBold^{\top} =
    \begin{bmatrix}
    \Delta\stateBold_{1}^{\top} & \Delta\stateBold_{2}^{\top} & \cdots & \Delta\stateBold_{n}^{\top}
    \end{bmatrix}
    \end{equation*}
    \item Coefficient vector
    \begin{equation*}
    \linearSystemb^{\top} =
    \begin{bmatrix}
    \overline{\linearSystemb}_{1}^{\top} & \overline{\linearSystemb}_{2}^{\top} & \cdots & \overline{\linearSystemb}_{n}^{\top}
    \end{bmatrix}
    \end{equation*}
    \item Matrix of normal equations
    \begin{equation*}
    \linearSystemH =
    \begin{bmatrix}
    \overline{\linearSystemH}_{11} & \overline{\linearSystemH}_{12} & \cdots & \overline{\linearSystemH}_{1n}\\
    \overline{\linearSystemH}_{21} & \overline{\linearSystemH}_{22} & \cdots & \overline{\linearSystemH}_{2n}\\
    \vdots & \vdots & \ddots & \vdots\\
    \overline{\linearSystemH}_{n1} & \overline{\linearSystemH}_{n2} & \cdots & \overline{\linearSystemH}_{nn}\\
   \end{bmatrix}
   \end{equation*}
   \end{itemize}
   
   \end{frame}
   
   \begin{frame}
   \frametitle{Construction of the linear system}
   \note{Information taken from https://youtu.be/uHbRKvD8TWg}
   For each constraint:
   \begin{itemize}
   \item Compute the error $\error_{ij} = t2v(\inverse{\observationBold}_{ij}(\inverse{\stateBold}_{i}\stateBold_{j}))$
   \item Compute the blocks of the Jacobians:
   \begin{equation*}
   \vec{A}_{ij} = \dfrac{\partial\error _{ij}(\stateBold _{i}, \stateBold _{j})}{\partial\stateBold _{i}} \quad \quad \vec{B}_{ij} = \dfrac{\partial\error _{ij}(\stateBold _{i}, \stateBold _{j})}{\partial\stateBold _{j}}
    \end{equation*}
    \item Update the coefficient vector:
    \begin{equation*}
    \overline{\linearSystemb}_{i}^{\top} += \error_{ij}^{\top}\informationMatrix_{ij}\vec{A}_{ij} \quad \quad \overline{\linearSystemb}_{j}^{\top} += \error_{ij}^{\top}\informationMatrix_{ij}\vec{B}_{ij}
    \end{equation*}
   
    \item Update the coefficient vector:
    \begin{align*}
    \overline{\linearSystemH}_{ij}^{\top} &+= \vec{A}_{ij}^{\top}\informationMatrix_{ij}\vec{A}_{ij} \quad \quad \overline{\linearSystemH}_{ij}^{\top} += \vec{A}_{ij}^{\top}\informationMatrix_{ij}\vec{B}_{ij} \\
    \overline{\linearSystemH}_{ij}^{\top} &+= \vec{B}_{ij}^{\top}\informationMatrix_{ij}\vec{A}_{ij} \quad \quad \overline{\linearSystemH}_{ij}^{\top} += \vec{B}_{ij}^{\top}\informationMatrix_{ij}\vec{B}_{ij}
    \end{align*}
    \end{itemize}
   
   \end{frame}
   
   \begin{frame}
    \frametitle{Algorithm}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
   
    \begin{algorithmic}[1]
    \Procedure{optimize}{$\stateBold$}
    \While (!converged)
    \State $(\linearSystemH,\linearSystemb) = buildLinearSystem(\stateBold)$
    \State $\vec{\Delta}\stateBold = solveSparse(\linearSystemH\vec{\Delta}\stateBold = -\linearSystemb)$
    \State $\stateBold = \stateBold + \vec{\Delta}\stateBold$
    \EndWhile
   
    \State \Return $\stateBold$
    \EndProcedure
    \end{algorithmic}
   
   \end{frame}
   
   
   \begin{frame}
    \frametitle{1D Trivial Example}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
   
    Two nodes and one observation
   
    \begin{figure}[!h]
    \includegraphics[width=0.2\textwidth]{pose_graph_1d_example.pdf}
    \end{figure}
   
    \small
   
    \begin{align*}
    \stateBold &=
    \begin{bmatrix}
    \stateBold_{1} & \stateBold_{2}
    \end{bmatrix}^{\top}
    =
    \begin{bmatrix}
    0 & 0
    \end{bmatrix}\\
    \observationBold_{12} &= 1\\
    \informationMatrix_{12} &= 2\\
    \error_{12} &= \observationBold_{12} - (\stateBold_{2} - \stateBold_{1}) = 1 - (0 - 0) = 1\\
    \jacobian_{12} &=
    \begin{bmatrix}
    1 & -1
    \end{bmatrix}\\
    \linearSystemb_{12}^{\top} &= \error_{12}^{\top} \informationMatrix_{12} \jacobian_{12} =
    \begin{bmatrix}
    2 & -2
    \end{bmatrix}\\
    \linearSystemH_{12} &= \jacobian_{12}^{\top} \informationMatrix_{12} \jacobian_{12} =
    \begin{bmatrix}
    2 & -2\\
    -2 & 2
    \end{bmatrix}\\
    \vec{\Delta}\stateBold &= -\inverse{\linearSystemH}_{12} \linearSystemb_{12}\\
    \end{align*}
   
    \begin{center}
    \alert{Problem:} $\det(\linearSystemH) = 0$, therefore we cannot invert $\linearSystemH$
    \end{center}
\end{frame}

\begin{frame}
    \frametitle{What's wrong?}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
    \begin{itemize}
    \item The constraint specifies a relative constraint between both nodes.
    \item Any pose of the nodes will be correct if the relative constraint between them is met. This problem is known as {\bf Gauge Freedom}
    \item To solve it, we have to {\bf fix} a node. By fixing a node, we are adding a {\bf Prior}!
    \end{itemize}
    
    \begin{align*}
    \linearSystemH &=
    \begin{bmatrix}
    2 & -2\\
    -2 & 2
    \end{bmatrix}
    +
    \begin{bmatrix}
    1 & 0\\
    0 & 0
    \end{bmatrix} \leftarrow \text{restriction that puts } \vec{\Delta}\stateBold_{1} = 0\\
    \vec{\Delta}\stateBold &= -\inverse{\linearSystemH}_{12} \linearSystemb_{12}\\
    \vec{\Delta}\stateBold &=
    \begin{bmatrix}
    0 & 1
    \end{bmatrix}^{\top}
    \end{align*}
    
    With this we make the update of node $\stateBold_{1}$ to be 0 and the update of $\stateBold_{2}$ is updated with 1.
    \end{frame}
    
    \begin{frame}
    \frametitle{Role of the Prior}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
    \begin{itemize}
    \item We saw that the information matrix $\linearSystemH$ is not of full rank, and therefore not invertible.
    \item A global reference frame has not been fixed.
    \item Fixing a global reference frame is strongly related to the prior $p(\stateBold_{0})$.
    \item A Gaussian estimate of $\stateBold_{0}$ results in adding a constraint.
    \item Example: The first pose must be fixed at the origin.
    \begin{equation*}
    \error(\stateBold_{0}) = t2v(\stateBold_{0})
    \end{equation*}
    That is, To minimize, we want the error term $\error(\stateBold_{0})$ to be as small as possible. This happens when $\stateBold_{0}$ tends to 0.
    \end{itemize}
    
    \end{frame}
    
    \begin{frame}
    \frametitle{Fixing a subset of variables}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
    \begin{itemize}
    \item We assume that the value of certain variables during the optimization is known a priori.
    \item We want to optimize all the other variables but keep them fixed.
    \item We can do as we did with the previous prior, but the problem is that it is only a {\bf soft constraint} and not really a fix. Another constraint may move this node, and therefore we cannot guarantee that it has a fixed value.
    \item \textbf{To effectively make a variable fixed, we must make it non-optimizable, and therefore we must remove it from the linear system}. Removing it from the linear system means it is not updated, and all other variables are constrained by it. This is done by constructing the entire linear system and then simply deleting the row and column corresponding to the variable. Finally, the linear system is solved. \item Column and row suppression works as a {\bf conditioning}, that is, it is a condition that says, "Since a node takes a value, the others are affected in this way."
    
    \end{itemize}
    
    \end{frame}
    
    \begin{frame}
    \frametitle{We can suppress the columns and rows of the corresponding variables}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
    \begin{itemize}
    \item The reason we can do this stems from {\bf conditioning on Gaussian distributions}.
    \item {\bf Conditioning in the information space} means that we can remove a portion of the information matrix, and this corresponds to a conditioning operation on the Gaussian distribution.
    \item The $\linearSystemH$ is an information matrix of our entire problem with all the constraints put together. Therefore, by removing a row and a column, we are conditioning the system by making them fixed and non-updatable, and by restricting all other variables.
    \end{itemize}
    
    \footnotetext{More info in the paper: Exactly sparse delayed-state filters for view-based SLAM. Eustice, Ryan M., Singh, Hanumant, Leonard, John J.}
    
\end{frame}

\begin{frame}
    \frametitle{Uncertainty}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
    \begin{itemize}
    \item $\linearSystemH$ represents the information matrix given the linearization point.
    \item The inverse of $\linearSystemH$ is the (dense) covariance matrix. Computing the inverse is computationally very expensive. However, we can compute parts of the covariance matrix.
    \item The blocks on the diagonal of the covariance matrix represent the uncertainties of the corresponding variables.
    \end{itemize}
    \end{frame}
    
    \begin{frame}
    \frametitle{Relative Uncertainty}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    \begin{itemize}
    \item Determine the relative uncertainty between $\stateBold_{i}$ and $\stateBold_{j}$.
    
    \begin{itemize}
    \item This is especially useful for cycle detection, as we can compute the uncertainty of the current pose relative to a previous pose. This allows us to determine whether a cycle is possible between poses or not (if the previous pose is within the covariance, then we detect a cycle).
    \end{itemize}
    
    \item To determine the relative uncertainty between $\stateBold_{i}$ and $\stateBold_{j}$ we will:
    \begin{itemize}
    \item Build the complete matrix $\linearSystemH$
    \item Delete the rows and columns of $\stateBold_{i}$ (equivalent to not optimizing/fixing the variable)
    \item Compute the $j,j$ block of the inverse
    \item This block contains the covariance matrix of $\stateBold_{j}$ with respect to $\stateBold_{i}$ which is fixed.
    
    \end{itemize}
    \end{itemize}
    
    \begin{figure}[!h]
    \includegraphics[width=0.2\textwidth]{pose_graph_relative_uncertainty.pdf}
    \end{figure}
    
    \end{frame}
    
    \begin{frame}
    \frametitle{Pose-Graph Summary}
    \note{Information taken from https://youtu.be/uHbRKvD8TWg}
    
    \begin{itemize}
    \item The back-end of the SLAM problem can be solved by applying Gauss-Newton
    \item The matrix $\linearSystemH$ is sparse
    \item That $\linearSystemH$ is sparse allows us to solve the linear system efficiently
    \end{itemize}
    
    \end{frame}
    
    \begin{frame}
    \frametitle{Graph-Based SLAM with Landmarks}
    \note{Information taken from https://youtu.be/mZBdPgBtrCM}
    
     \begin{figure}[!h]
     \includegraphics[width=0.6\textwidth]{images/pose_landmark_graph_example.pdf}
     \end{figure}
    
    \end{frame}
    
    \begin{frame}
     \frametitle{Landmark observations}
     \note{Information taken from https://youtu.be/mZBdPgBtrCM}
    
     \begin{tikzpicture}[remember picture,overlay]
     \node[xshift=-2.5cm,yshift=2cm] at (current page.east) { \includegraphics[height=0.5\textheight]{./images/landmark_observation_x_y_sensor.pdf}};
     \end{tikzpicture}
    \vspace{4em}
    \begin{itemize}
    \item Expected observation for an (x-y sensor\footnote{x-y sensor is a sensor whose measurements have the form (x,y), a point in the scene. In a 2D world.})
    \begin{figure}[!h]
    \includegraphics[width=0.6\textwidth]{images/pose_landmark_graph_expected_observation.pdf}
    \end{figure}
    \item Error function
    \begin{align*}
    \error_{ij}(\stateBold_{i}, \stateBold_{j}) &= \hat{\observationBold}_{ij} - \observationBold_{ij}\\
    &= \rotation_{i}^{\top}(\stateBold_{j}-\translation_{i}) - \observationBold_{ij}
     \end{align*}
     \end{itemize}
    
    \end{frame}
    
    \begin{frame}
     \frametitle{Bearing only observations (angle observations)}
     \note{Information taken from https://youtu.be/mZBdPgBtrCM}
    
     \begin{tikzpicture}[remember picture,overlay]
     \node[xshift=-2.5cm,yshift=2cm] at (current page.east) { \includegraphics[height=0.5\textheight]{./images/landmark_observation_bearing_only_sensor.pdf}};
     \end{tikzpicture}
    
    
     \begin{itemize}
     \item A landmark is a 2D point
     \item The robot observes the angle towards the landmark
     \item Observation function
     \begin{figure}[!h]
     \includegraphics[width=0.6\textwidth]{images/pose_landmark_graph_bearing_observation.pdf}
     \end{figure}
     \item Error function
     \begin{equation*}
     \error _{ij}(\stateBold _{i},\stateBold _{j}) = \arctan{\dfrac{(\stateBold _{j}-\translation _{i}).y}{(\stateBold _{j}-\translation _{i}).x}} - \theta _{i} - \observationBold _{j}
     \end{equation*}
     \end{itemize}
    
    
\end{frame}

\begin{frame}
    \frametitle{Rank of matrix H}
    \note{Information taken from https://youtu.be/mZBdPgBtrCM}
    
    \begin{itemize}
    \item What is the rank of the matrix $\linearSystemH_{ij}$ for a 2D pose-landmark constraint?
    \begin{itemize}
    \item The rank of $\linearSystemH_{ij}$ is determined by the rank of the Jacobian $\jacobian_{ij}$ which is at most a $2 \times 5$ matrix (2 because the measurement gives 2D information and 5 for $\begin{bmatrix} x & y & \theta & l_{x} & l_{y} \end{bmatrix}$)
    \item $\linearSystemH_{ij}$ cannot have more than rank 2
    
    $rank(\linearSystemH_{ij}) = rank(\jacobian_{ij}^{\top} \informationMatrix_{ij} \jacobian_{ij}) = rank(\jacobian_{ij}) \quad \text{See: The Matrix Cookcook sec. 9.6.9}$
    \end{itemize}
    
    \item What is the rank of the matrix $\linearSystemH_{ij}$ for a pose-landmark bearing-only constraint?
    \begin{itemize}
    \item The rank of $\linearSystemH_{ij}$ is determined by the rank of the Jacobian $\jacobian_{ij}$ which is at most a $1 \times 5$ matrix. 1 because the measurement gives 1D information and 5 for $\begin{bmatrix} x & y & \theta & l_{x} & l_{y} \end{bmatrix}$
    \item $\linearSystemH_{ij}$ has rank 1
    \end{itemize}
    
    \end{itemize}
    \end{frame}
    
    \begin{frame}
    \frametitle{Where is the robot?}
    \note{Information taken from https://youtu.be/mZBdPgBtrCM}
    \begin{itemize}
    \item The robot observes a landmark $(x,y)$
    \item Where can the robot be relative to the landmark?
    \end{itemize}
    
    \only<1>{
    \begin{center}
    \includegraphics[width=0.3\textwidth]{images/robot_pose_landmark_with_xy_sensor1.pdf}
    \end{center}
    }
    
    \only<2>{
    \begin{center}
    \includegraphics[width=0.3\textwidth]{images/robot_pose_landmark_with_xy_sensor2.pdf}
    \end{center}
    The robot can be anywhere on the circle.\\
    It is a 1D solution space (restricted by the robot's distance and orientation).
    }
    \end{frame}
    
    \begin{frame}
    \frametitle{Where is the robot?}
    \note{Information taken from https://youtu.be/mZBdPgBtrCM}
    \begin{itemize}
    \item The robot observes a landmark (bearing-only)
    \item Where can the robot be relative to the landmark?
    \end{itemize}
    
    \only<1>{
    \begin{center}
    \includegraphics[width=0.3\textwidth]{images/robot_pose_landmark_with_bearing_sensor1.pdf}
    \end{center}
    }
    
    \only<2>{
    \begin{center}
    \includegraphics[width=0.3\textwidth]{images/robot_pose_landmark_with_bearing_sensor2.pdf}
    \end{center}
    The robot can be anywhere in the xy plane. It will always be pointing toward the landmark.
    It is a 2D solution space (restricted by the robot's orientation).
    }
    \end{frame}
    
    \begin{frame}
    \frametitle{Range}
    \note{Information taken from https://youtu.be/mZBdPgBtrCM}
    \begin{itemize}
    \item In SLAM with landmarks, the system can be indeterminate
    \item The rank of $\linearSystemH$ is {\bf less than or equal} to the sum of the ranks of all observations
    \item To determine a {\bf unique solution}, the system must have {\bf full rank}
    \end{itemize}
    
    \only<2-> {
    Questions:
    }
    
    \begin{itemize}
    \item<2-> How many landmark observations $(x,y)$ are needed to solve a robot's pose?
    \begin{itemize}
    \item<3-> At least 2 observations are required
    \end{itemize}
    \item<4-> How many bearing-only observations are needed to resolve a robot's pose?
    \begin{itemize}
    \item<5-> At least 3 observations are required
    \end{itemize}
    \end{itemize}
    \end{frame}
    
    \begin{frame}
    \frametitle{Indeterminate System}
    \note{Information taken from https://youtu.be/mZBdPgBtrCM}
    \begin{itemize}
    \item There is no guarantee that a system has full range
    \begin{itemize}
    \item Landmarks can only be observed once
    \item The robot may not have odometry information. \note{Odometry in general provides information about the entire pose.}
    \end{itemize}
    \item We can deal with these problems by using a \emph{damping factor} for $\linearSystemH$
    \item Instead of solving $\linearSystemH \vec{\Delta} \stateBold = - \linearSystemb$, we solve
    $(\linearSystemH + \lambda \vec{I}) \vec{\Delta} \stateBold = - \linearSystemb$
     \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Levenberg–Marquardt}
    \note{Información extraída de https://youtu.be/mZBdPgBtrCM}
    
    \footnotesize
    
    \begin{itemize}
        \item The damping factor $\lambda \vec{I}$ makes the system positive definite.
        \item It is a weighted sum of the Gauss-Newton method and the gradient descent method. \note{When lambda increases, H becomes negligible and Levenberg–Marquardt behaves like the gradient method. This is because the vector b has information from the Jacobian, and it could be solved to give the gradient descent equation. On the other hand, when lambda is very small, Levenberg–Marquardt behaves like the Gauss-Newton method. Intuitively, Levenberg–Marquardt behaves like Gauss–Newton when it is close to the minimum (lambda decreases) and like gradient descent when it is far from the minimum (lambda increases).}
        \item The damping factor regulates convergence using backup/restore actions.
        \end{itemize}
        
    \begin{algorithmic}[1]
        \Procedure{Levenberg–Marquardt}{$\stateBold$} \Comment{$\stateBold$: initial seed}
        \While (!converged)
        \State $\lambda =  \lambda_{\textrm{init}}$
        \State $<\linearSystemH, \linearSystemb> = buildLinearSystem(\stateBold)$
        \State $E = error(\stateBold)$
        \State $\stateBold_{\textrm{old}} = \stateBold$
        \State $\vec{\Delta}\stateBold = solveSparse((\linearSystemH + \lambda \vec{I}) \vec{\Delta} \stateBold = - \linearSystemb)$
        \State $\stateBold \mathrel{+}= \vec{\Delta}\stateBold$
        \If{$E < error(\stateBold)$}
        \State $\stateBold = \stateBold_{\textrm{old}}$
        \State $\lambda \mathrel{*}= 2$
        \Else
        \State $\lambda \mathrel{/}= 2$
        \EndIf
        \EndWhile
        \EndProcedure
    \end{algorithmic}

\end{frame}
